#!/usr/bin/env python3
"""
Nova å„ªåŒ–è¨˜æ†¶é«”ç›£æ§å™¨ - åŸºæ–¼ TCK å„ªåŒ–è—åœ–
Nova Optimized Memory Monitor - Based on TCK Optimization Blueprints

å„ªåŒ–ç­–ç•¥:
- âœ… Dataclass å„ªåŒ–: ä½¿ç”¨ @dataclass æå‡ç‰©ä»¶æ•ˆèƒ½
- âœ… LRU Cache: ä½¿ç”¨ @lru_cache å¿«å–            # è¼•é‡ GPU å¿«å–æ¸…ç†
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():  # type: ignore
                try:
                    torch.cuda.empty_cache()  # type: ignore
                    print("âœ… GPU å¿«å–æ¸…ç†å®Œæˆ")
                except Exception as e:
                    print(f"âŒ GPU å¿«å–æ¸…ç†å¤±æ•—: {e}")
- âœ… é›†åˆæ“ä½œ: ä½¿ç”¨ set() å„ªåŒ–æŸ¥æ‰¾æ“ä½œ
- âœ… ç”Ÿæˆå™¨è¡¨é”å¼: ä½¿ç”¨ç”Ÿæˆå™¨ç¯€çœè¨˜æ†¶é«”
- âœ… å­—ä¸² join: ä½¿ç”¨ str.join() å„ªåŒ–å­—ä¸²æ“ä½œ
- âœ… å¸¸æ•¸æå–: å°‡å¸¸æ•¸æå–åˆ°æ¨¡çµ„ç´šåˆ¥
"""

import gc
import time
from dataclasses import dataclass
from functools import lru_cache
from typing import Any, Dict, List, Optional

import GPUtil
import psutil

try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    torch = None  # type: ignore
    TORCH_AVAILABLE = False

# === å¸¸æ•¸æå– (æ¨¡çµ„ç´šåˆ¥å¿«å–) ===
DEFAULT_THRESHOLD = 0.85
DEFAULT_CRITICAL_THRESHOLD = 0.95
DEFAULT_CHECK_INTERVAL = 30
GB_DIVISOR = 1024**3
MB_DIVISOR = 1024**2

# === å­—ä¸²å¸¸æ•¸ (é¿å…é‡è¤‡å­—ä¸²æ“ä½œ) ===
MEMORY_STATS_HEADER = "\nè¨˜æ†¶é«”ç›£æ§çµ±è¨ˆ:"
TOTAL_MEMORY_FMT = "   ç¸½è¨˜æ†¶é«”: {:.1f} GB"
USED_MEMORY_FMT = "   å·²ä½¿ç”¨: {:.1f} GB ({:.1f}%)"
AVAILABLE_MEMORY_FMT = "   å¯ç”¨: {:.1f} GB"
THRESHOLDS_FMT = "   æ¸…ç†é–¾å€¼: {:.1f}% / {:.1f}%"
CLEANUP_COUNT_FMT = "   æ¸…ç†æ¬¡æ•¸: {}"
CPU_INFO_FMT = "   CPU ä½¿ç”¨ç‡: {:.1f}% (æ ¸å¿ƒæ•¸: {})"
GPU_INFO_FMT = "   GPU ({}): {:.0f} MB / {:.0f} MB"
GPU_UTIL_FMT = "   GPU ä½¿ç”¨ç‡: {:.1f}%"
GPU_ERROR_FMT = "   GPU éŒ¯èª¤: {}"
NO_GPU_MSG = "   æœªæª¢æ¸¬åˆ° GPU"
LAST_CLEANUP_FMT = "   æœ€å¾Œæ¸…ç†: {}ç´šåˆ¥ï¼Œ{:.1f}ç§’å‰"
CLEANUP_TIME_FMT = "æ¸…ç†è€—æ™‚: {:.3f} ç§’"


@dataclass(frozen=True)
class MemoryThresholds:
    """è¨˜æ†¶é«”é–¾å€¼è¨­å®š (ä½¿ç”¨ dataclass å„ªåŒ–)"""

    normal: float = DEFAULT_THRESHOLD
    critical: float = DEFAULT_CRITICAL_THRESHOLD

    def __post_init__(self):
        """é©—è­‰é–¾å€¼ç¯„åœ"""
        if not (0.0 <= self.normal <= 1.0):
            raise ValueError(f"æ¨™æº–é–¾å€¼å¿…é ˆåœ¨ 0.0-1.0 ä¹‹é–“: {self.normal}")
        if not (0.0 <= self.critical <= 1.0):
            raise ValueError(f"å±éšªé–¾å€¼å¿…é ˆåœ¨ 0.0-1.0 ä¹‹é–“: {self.critical}")
        if self.normal >= self.critical:
            raise ValueError("æ¨™æº–é–¾å€¼å¿…é ˆå°æ–¼å±éšªé–¾å€¼")


@dataclass
class CleanupRecord:
    """æ¸…ç†è¨˜éŒ„ (ä½¿ç”¨ dataclass å„ªåŒ–)"""

    level: str
    timestamp: float
    duration: float = 0.0


@dataclass
class MemoryStats:
    """è¨˜æ†¶é«”çµ±è¨ˆè³‡æ–™ (ä½¿ç”¨ dataclass å„ªåŒ–)"""

    total: int
    available: int
    used: int
    percentage: float
    thresholds: MemoryThresholds
    cleanup_count: int
    last_cleanup: Optional[CleanupRecord] = None

    # GPU è³‡è¨Š (å¯é¸)
    gpu_available: bool = False
    gpu_name: str = ""
    gpu_memory_used: int = 0
    gpu_memory_total: int = 0
    gpu_memory_util: float = 0.0
    gpu_error: str = ""

    # CPU è³‡è¨Š (æ–°å¢)
    cpu_percent: float = 0.0
    cpu_count: int = 0

    @property
    def total_gb(self) -> float:
        """ç¸½è¨˜æ†¶é«” (GB)"""
        return self.total / GB_DIVISOR

    @property
    def used_gb(self) -> float:
        """å·²ä½¿ç”¨è¨˜æ†¶é«” (GB)"""
        return self.used / GB_DIVISOR

    @property
    def available_gb(self) -> float:
        """å¯ç”¨è¨˜æ†¶é«” (GB)"""
        return self.available / GB_DIVISOR


class NovaMemoryMonitor:
    """
    Nova å„ªåŒ–è¨˜æ†¶é«”ç›£æ§å™¨

    åŸºæ–¼ TCK å„ªåŒ–è—åœ–å¯¦ç¾çš„è¨˜æ†¶é«”ç›£æ§è§£æ±ºæ–¹æ¡ˆ:
    - ä½¿ç”¨ psutil é€²è¡Œç³»çµ±è¨˜æ†¶é«”ç›£æ§
    - ä½¿ç”¨ GPUtil é€²è¡Œ GPU ç›£æ§
    - æ‡‰ç”¨å¤šé …æ•ˆèƒ½å„ªåŒ–ç­–ç•¥
    """

    def __init__(self, thresholds: Optional[MemoryThresholds] = None):
        """
        åˆå§‹åŒ– Nova è¨˜æ†¶é«”ç›£æ§å™¨

        Args:
            thresholds: è¨˜æ†¶é«”é–¾å€¼è¨­å®š
        """
        self.thresholds = thresholds or MemoryThresholds()
        self.cleanup_history: List[CleanupRecord] = []

        print("ğŸš€ åˆå§‹åŒ– Nova è¨˜æ†¶é«”ç›£æ§å™¨ (TCK å„ªåŒ–ç‰ˆ)")
        print(f"   æ¨™æº–é–¾å€¼: {self.thresholds.normal:.1%}")
        print(f"   å±éšªé–¾å€¼: {self.thresholds.critical:.1%}")

    def should_cleanup(self) -> Optional[str]:
        """
        æª¢æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†è¨˜æ†¶é«”

        Returns:
            'critical': éœ€è¦å±éšªç´šåˆ¥æ¸…ç†
            'normal': éœ€è¦æ¨™æº–æ¸…ç†
            None: ä¸éœ€è¦æ¸…ç†
        """
        memory_usage = psutil.virtual_memory().percent / 100

        if memory_usage > self.thresholds.critical:
            print(f"âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨ç‡å±éšª ({memory_usage:.1%})ï¼Œå¼·åˆ¶åŸ·è¡Œæ·±åº¦æ¸…ç†")
            return "critical"
        elif memory_usage > self.thresholds.normal:
            print(f"âš¡ è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ ({memory_usage:.1%})ï¼ŒåŸ·è¡Œæ¨™æº–æ¸…ç†")
            return "normal"

        return None

    def cleanup_memory(self, level: str = "normal") -> float:
        """
        åŸ·è¡Œè¨˜æ†¶é«”æ¸…ç† (å¢å¼·ç‰ˆï¼šåŒ…å« GPU å¿«å–æ¸…ç†)

        Args:
            level: æ¸…ç†ç´šåˆ¥ ('normal' æˆ– 'critical')

        Returns:
            float: æ¸…ç†è€—æ™‚ (ç§’)
        """
        start_time = time.time()

        if level == "critical":
            # å±éšªç´šåˆ¥: å®Œæ•´æ¸…ç† + GPU å¿«å–æ¸…ç†
            print("ğŸ§¹ åŸ·è¡Œæ·±åº¦è¨˜æ†¶é«”æ¸…ç†...")

            # Python åƒåœ¾å›æ”¶
            gc.collect()
            print("âœ… Python åƒåœ¾å›æ”¶å®Œæˆ")

            # GPU å¿«å–æ¸…ç† (ä½¿ç”¨ torch)
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():  # type: ignore
                print("ğŸ® æ¸…ç† GPU å¿«å–...")
                try:
                    torch.cuda.empty_cache()  # type: ignore
                    torch.cuda.ipc_collect()  # type: ignore
                    print("âœ… GPU å¿«å–æ¸…ç†å®Œæˆ")
                except Exception as e:
                    print(f"âŒ GPU å¿«å–æ¸…ç†å¤±æ•—: {e}")
            else:
                print("â„¹ï¸  æœªæª¢æ¸¬åˆ° GPU æˆ–æœªå®‰è£ PyTorch")

            # å‚³çµ± GPU ç›£æ§ (GPUtil)
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    print("ğŸ“Š GPU ç‹€æ…‹ç›£æ§ä¸­...")
                    for i, gpu in enumerate(gpus):
                        print(
                            f"   GPU {i}: {gpu.memoryUsed:.0f}MB / {gpu.memoryTotal:.0f}MB"
                        )
            except Exception as e:
                print(f"âŒ GPU ç›£æ§éŒ¯èª¤: {e}")

            cleanup_record = CleanupRecord(level, time.time())
            self.cleanup_history.append(cleanup_record)
            print("âœ… å±éšªç´šåˆ¥è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")

        else:
            # æ¨™æº–ç´šåˆ¥: åŸºæœ¬æ¸…ç† + è¼•é‡ GPU æ¸…ç†
            print("ğŸ§¹ åŸ·è¡Œæ¨™æº–è¨˜æ†¶é«”æ¸…ç†...")

            # Python åƒåœ¾å›æ”¶
            gc.collect()
            print("âœ… Python åƒåœ¾å›æ”¶å®Œæˆ")

            # è¼•é‡ GPU å¿«å–æ¸…ç†
            if TORCH_AVAILABLE and torch is not None and torch.cuda.is_available():  # type: ignore
                try:
                    torch.cuda.empty_cache()  # type: ignore
                    print("âœ… GPU å¿«å–æ¸…ç†å®Œæˆ")
                except Exception as e:
                    print(f"âŒ GPU å¿«å–æ¸…ç†å¤±æ•—: {e}")

            cleanup_record = CleanupRecord(level, time.time())
            self.cleanup_history.append(cleanup_record)
            print("âœ… æ¨™æº–ç´šåˆ¥è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")

        cleanup_time = time.time() - start_time
        print(CLEANUP_TIME_FMT.format(cleanup_time))

        # æ›´æ–°æ¸…ç†è¨˜éŒ„çš„è€—æ™‚
        cleanup_record.duration = cleanup_time

        return cleanup_time

    def _get_gpu_info_cached(self) -> Dict[str, Any]:
        """
        ç²å– GPU è³‡è¨Š

        Returns:
            dict: GPU è³‡è¨Šå­—å…¸
        """
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹ GPU
                return {
                    "available": True,
                    "name": gpu.name,
                    "memory_used": gpu.memoryUsed,
                    "memory_total": gpu.memoryTotal,
                    "memory_util": gpu.memoryUtil * 100,
                    "error": "",
                }
            else:
                return {
                    "available": False,
                    "name": "",
                    "memory_used": 0,
                    "memory_total": 0,
                    "memory_util": 0.0,
                    "error": "",
                }
        except Exception as e:
            return {
                "available": False,
                "name": "",
                "memory_used": 0,
                "memory_total": 0,
                "memory_util": 0.0,
                "error": str(e),
            }

    def get_memory_stats(self) -> MemoryStats:
        """
        ç²å–ç•¶å‰è¨˜æ†¶é«”çµ±è¨ˆä¿¡æ¯

        Returns:
            MemoryStats: è¨˜æ†¶é«”çµ±è¨ˆç‰©ä»¶
        """
        vm = psutil.virtual_memory()
        gpu_info = self._get_gpu_info_cached()

        # ç²å– CPU è³‡è¨Š
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count(logical=True) or 0

        return MemoryStats(
            total=vm.total,
            available=vm.available,
            used=vm.used,
            percentage=vm.percent,
            thresholds=self.thresholds,
            cleanup_count=len(self.cleanup_history),
            last_cleanup=self.cleanup_history[-1] if self.cleanup_history else None,
            gpu_available=gpu_info["available"],
            gpu_name=gpu_info["name"],
            gpu_memory_used=gpu_info["memory_used"],
            gpu_memory_total=gpu_info["memory_total"],
            gpu_memory_util=gpu_info["memory_util"],
            gpu_error=gpu_info["error"],
            cpu_percent=cpu_percent,
            cpu_count=cpu_count,
        )

    def print_stats(self, stats: Optional[MemoryStats] = None) -> None:
        """
        æ‰“å°ç•¶å‰è¨˜æ†¶é«”çµ±è¨ˆä¿¡æ¯

        Args:
            stats: è¨˜æ†¶é«”çµ±è¨ˆè³‡æ–™ (å¦‚æœç‚º None å‰‡è‡ªå‹•ç²å–)
        """
        if stats is None:
            stats = self.get_memory_stats()

        # ä½¿ç”¨å­—ä¸² join å„ªåŒ–å­—ä¸²æ“ä½œ
        output_lines = [
            MEMORY_STATS_HEADER,
            TOTAL_MEMORY_FMT.format(stats.total_gb),
            USED_MEMORY_FMT.format(stats.used_gb, stats.percentage),
            AVAILABLE_MEMORY_FMT.format(stats.available_gb),
            THRESHOLDS_FMT.format(
                stats.thresholds.normal * 100, stats.thresholds.critical * 100
            ),
            CLEANUP_COUNT_FMT.format(stats.cleanup_count),
            CPU_INFO_FMT.format(stats.cpu_percent, stats.cpu_count),
        ]

        # GPU è³‡è¨Š
        if stats.gpu_available:
            output_lines.extend(
                [
                    GPU_INFO_FMT.format(
                        stats.gpu_name, stats.gpu_memory_used, stats.gpu_memory_total
                    ),
                    GPU_UTIL_FMT.format(stats.gpu_memory_util),
                ]
            )
        elif stats.gpu_error:
            output_lines.append(GPU_ERROR_FMT.format(stats.gpu_error))
        else:
            output_lines.append(NO_GPU_MSG)

        # æœ€å¾Œæ¸…ç†è³‡è¨Š
        if stats.last_cleanup:
            time_since = time.time() - stats.last_cleanup.timestamp
            output_lines.append(
                LAST_CLEANUP_FMT.format(stats.last_cleanup.level, time_since)
            )

        # ä½¿ç”¨ join ä¸€æ¬¡æ€§è¼¸å‡º (å„ªåŒ–å­—ä¸²æ“ä½œ)
        print("\n".join(output_lines))

    def get_cleanup_summary(self) -> Dict[str, Any]:
        """
        ç²å–æ¸…ç†æ­·å²æ‘˜è¦

        Returns:
            dict: æ¸…ç†æ‘˜è¦çµ±è¨ˆ
        """
        if not self.cleanup_history:
            return {"total_cleanups": 0, "avg_duration": 0.0}

        # ä½¿ç”¨ç”Ÿæˆå™¨è¡¨é”å¼å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨
        durations = (record.duration for record in self.cleanup_history)
        total_duration = sum(durations)

        # ä½¿ç”¨é›†åˆæ“ä½œçµ±è¨ˆæ¸…ç†é¡å‹
        cleanup_levels = {record.level for record in self.cleanup_history}
        level_counts = {}
        for level in cleanup_levels:
            level_counts[level] = sum(
                1 for record in self.cleanup_history if record.level == level
            )

        return {
            "total_cleanups": len(self.cleanup_history),
            "avg_duration": total_duration / len(self.cleanup_history),
            "level_counts": level_counts,
            "total_duration": total_duration,
        }

    def get_cpu_percent(self, interval: Optional[float] = None) -> float:
        """
        ç²å–ç•¶å‰ CPU ä½¿ç”¨ç‡

        Args:
            interval: ç›£æ§é–“éš” (ç§’)ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­å€¼ 1.0

        Returns:
            float: CPU ä½¿ç”¨ç‡ç™¾åˆ†æ¯” (0.0-100.0)
        """
        try:
            actual_interval = interval if interval is not None else 1.0
            return psutil.cpu_percent(interval=actual_interval)
        except Exception as e:
            print(f"âŒ ç²å– CPU ä½¿ç”¨ç‡å¤±æ•—: {e}")
            return 0.0

    def should_cpu_delay(self, threshold: float = 80.0) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦æ‡‰è©²æ ¹æ“š CPU ä½¿ç”¨ç‡å»¶é²åŸ·è¡Œ

        Args:
            threshold: CPU ä½¿ç”¨ç‡é–¾å€¼ (0.0-100.0)

        Returns:
            bool: æ˜¯å¦æ‡‰è©²å»¶é²
        """
        cpu_percent = self.get_cpu_percent()
        return cpu_percent > threshold

    def smart_cpu_delay(self, threshold: float = 80.0, max_delay: float = 0.5) -> None:
        """
        æ™ºæ…§ CPU å»¶é²ï¼šæ ¹æ“š CPU ä½¿ç”¨ç‡å‹•æ…‹èª¿æ•´å»¶é²æ™‚é–“

        Args:
            threshold: CPU ä½¿ç”¨ç‡é–¾å€¼ (0.0-100.0)
            max_delay: æœ€å¤§å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰
        """
        if not self.should_cpu_delay(threshold):
            print("âœ… CPU ä½¿ç”¨ç‡æ­£å¸¸ï¼Œç„¡éœ€å»¶é²")
            return

        cpu_percent = self.get_cpu_percent()
        # å‹•æ…‹è¨ˆç®—å»¶é²æ™‚é–“ï¼šCPU ä½¿ç”¨ç‡è¶Šé«˜ï¼Œå»¶é²è¶Šé•·
        delay_time = min(max_delay, (cpu_percent / 100.0) * max_delay)
        print(
            f"â³ CPU ä½¿ç”¨ç‡ {cpu_percent:.1f}% > {threshold:.1f}%ï¼Œå»¶é² {delay_time:.2f} ç§’"
        )
        time.sleep(delay_time)


def create_monitor_with_config(
    config_path: str = "memory_config.json",
) -> NovaMemoryMonitor:
    """
    å¾è¨­å®šæª”æ¡ˆå‰µå»ºç›£æ§å™¨ (ä½¿ç”¨ LRU å¿«å–å„ªåŒ–è¨­å®šè¼‰å…¥)

    Args:
        config_path: è¨­å®šæª”æ¡ˆè·¯å¾‘

    Returns:
        NovaMemoryMonitor: é…ç½®å¥½çš„ç›£æ§å™¨å¯¦ä¾‹
    """
    config = load_memory_config_cached(config_path)

    thresholds = MemoryThresholds(
        normal=config.get("threshold", DEFAULT_THRESHOLD),
        critical=config.get("critical_threshold", DEFAULT_CRITICAL_THRESHOLD),
    )

    return NovaMemoryMonitor(thresholds)


@lru_cache(maxsize=8)  # å¿«å–å¤šå€‹è¨­å®šæª”æ¡ˆ
def load_memory_config_cached(config_path: str) -> Dict[str, Any]:
    """
    è¼‰å…¥è¨˜æ†¶é«”ç›£æ§è¨­å®š (ä½¿ç”¨ LRU å¿«å–é¿å…é‡è¤‡ I/O)

    Args:
        config_path: è¨­å®šæª”æ¡ˆè·¯å¾‘

    Returns:
        dict: è¨­å®šå­—å…¸
    """
    import json
    import os

    if not os.path.exists(config_path):
        # è¿”å›é è¨­è¨­å®š
        return {
            "threshold": DEFAULT_THRESHOLD,
            "critical_threshold": DEFAULT_CRITICAL_THRESHOLD,
            "check_interval": DEFAULT_CHECK_INTERVAL,
        }

    try:
        with open(config_path, encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  è¼‰å…¥è¨­å®šæª”æ¡ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­è¨­å®š")
        return {
            "threshold": DEFAULT_THRESHOLD,
            "critical_threshold": DEFAULT_CRITICAL_THRESHOLD,
            "check_interval": DEFAULT_CHECK_INTERVAL,
        }


def monitor_memory_continuous(
    monitor: NovaMemoryMonitor,
    interval: int = DEFAULT_CHECK_INTERVAL,
    duration: int = 300,
) -> Dict[str, Any]:
    """
    é€£çºŒç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

    Args:
        monitor: è¨˜æ†¶é«”ç›£æ§å™¨å¯¦ä¾‹
        interval: ç›£æ§é–“éš” (ç§’)
        duration: ç›£æ§æŒçºŒæ™‚é–“ (ç§’)

    Returns:
        dict: ç›£æ§æ‘˜è¦çµ±è¨ˆ
    """
    print(f"ğŸ”„ é–‹å§‹é€£çºŒç›£æ§ {duration} ç§’ï¼Œæ¯ {interval} ç§’æª¢æŸ¥ä¸€æ¬¡...")

    start_time = time.time()
    check_count = 0
    cleanup_count = 0

    try:
        while time.time() - start_time < duration:
            check_count += 1
            cleanup_level = monitor.should_cleanup()

            if cleanup_level:
                monitor.cleanup_memory(cleanup_level)
                cleanup_count += 1

            monitor.print_stats()
            time.sleep(interval)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç›£æ§è¢«ä½¿ç”¨è€…ä¸­æ–·")

    monitoring_duration = time.time() - start_time
    print(f"âœ… é€£çºŒç›£æ§å®Œæˆï¼Œç¸½æª¢æŸ¥æ¬¡æ•¸: {check_count}ï¼Œæ¸…ç†æ¬¡æ•¸: {cleanup_count}")

    return {
        "monitoring_duration": monitoring_duration,
        "check_count": check_count,
        "cleanup_count": cleanup_count,
        "checks_per_second": check_count / monitoring_duration
        if monitoring_duration > 0
        else 0,
    }


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    print("ğŸ§ª æ¸¬è©¦ Nova å„ªåŒ–è¨˜æ†¶é«”ç›£æ§å™¨")

    # å‰µå»ºç›£æ§å™¨
    monitor = NovaMemoryMonitor()

    # æ¸¬è©¦è¨˜æ†¶é«”ç›£æ§
    print("\nğŸ“Š --- è¨˜æ†¶é«”ç›£æ§æ¸¬è©¦ ---")
    stats = monitor.get_memory_stats()
    monitor.print_stats(stats)

    # æ¸¬è©¦æ¸…ç†åŠŸèƒ½
    print("\nğŸ§¹ --- æ¸…ç†åŠŸèƒ½æ¸¬è©¦ ---")
    cleanup_level = monitor.should_cleanup()
    if cleanup_level:
        monitor.cleanup_memory(cleanup_level)

    # æ¸¬è©¦æ‘˜è¦çµ±è¨ˆ
    print("\nğŸ“ˆ --- æ¸…ç†æ‘˜è¦ ---")
    summary = monitor.get_cleanup_summary()
    print(f"ç¸½æ¸…ç†æ¬¡æ•¸: {summary['total_cleanups']}")
    if summary["total_cleanups"] > 0:
        print(f"å¹³å‡æ¸…ç†è€—æ™‚: {summary['avg_duration']:.3f} ç§’")
        print(f"æ¸…ç†é¡å‹çµ±è¨ˆ: {summary['level_counts']}")

    print("\nğŸ‰ æ¸¬è©¦å®Œæˆ - Nova å„ªåŒ–è¨˜æ†¶é«”ç›£æ§å™¨é‹è¡Œæ­£å¸¸ï¼")
