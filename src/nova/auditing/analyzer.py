#!/usr/bin/env python3
"""
ğŸ” Nova AI ä»£ç¢¼åˆ†æå·¥å…·
=====================================
æ•´åˆä»£ç¢¼çµæ§‹åˆ†æå’Œä¾è³´é—œä¿‚æƒæ
"""

import ast
import importlib
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List


class CodeAnalyzer:
    """ä»£ç¢¼åˆ†æå™¨ - æ•´åˆçµæ§‹åˆ†æå’Œä¾è³´æƒæ"""

    def __init__(self, root_dir: str = "."):
        self.root_dir = Path(root_dir)
        self.python_files = []
        self.analysis_results = {}
        self.dependency_map = {}
        self.refactor_suggestions = []
        self.missing_imports = []
        self.circular_imports = []
        self.local_modules = set()

    def scan_python_files(self) -> List[Path]:
        """æƒææ‰€æœ‰Pythonæ–‡ä»¶"""
        print("æƒæPythonæ–‡ä»¶...")

        # æƒææ ¹ç›®éŒ„å’Œcoreç›®éŒ„
        patterns = ["*.py", "core/*.py", "tools/*.py"]
        py_files = []

        for pattern in patterns:
            py_files.extend(self.root_dir.glob(pattern))

        self.python_files = sorted(py_files)

        print(f"   ç™¼ç¾ {len(self.python_files)} å€‹Pythonæ–‡ä»¶")
        for file in self.python_files:
            print(f"   {file.relative_to(self.root_dir)}")
            # è¨˜éŒ„æœ¬åœ°æ¨¡çµ„åç¨±
            self.local_modules.add(file.stem)

        return self.python_files

    def analyze_file_complexity(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå–®å€‹æ–‡ä»¶çš„è¤‡é›œåº¦"""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)

            analysis = {
                "file_path": str(file_path.relative_to(self.root_dir)),
                "lines_of_code": len(content.splitlines()),
                "classes": [],
                "functions": [],
                "imports": {
                    "standard_library": [],
                    "third_party": [],
                    "local_imports": [],
                    "relative_imports": [],
                },
                "complexity_score": 0,
            }

            # åˆ†æé¡åˆ¥
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = {
                        "name": node.name,
                        "methods": [],
                        "line_start": node.lineno,
                        "docstring": ast.get_docstring(node) or "",
                    }

                    for item in node.body:
                        if isinstance(item, ast.FunctionDef):
                            class_info["methods"].append(
                                {
                                    "name": item.name,
                                    "line_start": item.lineno,
                                    "args_count": len(item.args.args),
                                }
                            )

                    analysis["classes"].append(class_info)

                elif isinstance(node, ast.FunctionDef) and node.col_offset == 0:
                    # åªè¨˜éŒ„é ‚å±¤å‡½æ•¸
                    function_info = {
                        "name": node.name,
                        "line_start": node.lineno,
                        "args_count": len(node.args.args),
                        "docstring": ast.get_docstring(node) or "",
                    }
                    analysis["functions"].append(function_info)

                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    # åˆ†æå°å…¥
                    self._analyze_import_node(node, analysis["imports"])

            # è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸
            analysis["complexity_score"] = self._calculate_complexity_score(analysis)

            return analysis

        except Exception as e:
            return {
                "file_path": str(file_path.relative_to(self.root_dir)),
                "error": str(e),
                "lines_of_code": 0,
                "complexity_score": 0,
            }

    def _analyze_import_node(self, node, imports_dict):
        """åˆ†æå°å…¥ç¯€é»"""
        if isinstance(node, ast.Import):
            for alias in node.names:
                module_name = alias.name
                self._categorize_import(module_name, imports_dict)

        elif isinstance(node, ast.ImportFrom):
            if node.module:
                if node.level > 0:  # ç›¸å°å°å…¥
                    imports_dict["relative_imports"].append(
                        {
                            "module": node.module or "",
                            "level": node.level,
                            "names": [alias.name for alias in node.names],
                        }
                    )
                else:
                    self._categorize_import(node.module, imports_dict)

    def _categorize_import(self, module_name: str, imports_dict):
        """åˆ†é¡å°å…¥æ¨¡çµ„ - TCKå„ªåŒ–ï¼šä½¿ç”¨é›†åˆæŸ¥æ‰¾åŠ é€Ÿ"""
        # TCKå„ªåŒ–ï¼šé å…ˆå°‡æ¨¡çµ„é›†åˆè½‰æ›ç‚ºé›†åˆä»¥å¯¦ç¾O(1)æŸ¥æ‰¾
        standard_modules = {
            "os",
            "sys",
            "json",
            "time",
            "datetime",
            "pathlib",
            "typing",
            "ast",
            "importlib",
            "traceback",
            "collections",
            "re",
        }

        third_party_modules = {
            "torch",
            "transformers",
            "numpy",
            "pandas",
            "matplotlib",
            "requests",
            "flask",
            "django",
            "scipy",
            "sklearn",
        }

        root_module = module_name.split(".")[0]

        # TCKå„ªåŒ–ï¼šé›†åˆæŸ¥æ‰¾ O(1) è€Œéåˆ—è¡¨æŸ¥æ‰¾ O(n)
        if root_module in standard_modules:
            imports_dict["standard_library"].append(module_name)
        elif root_module in third_party_modules:
            imports_dict["third_party"].append(module_name)
        elif (
            root_module in self.local_modules
            or module_name.startswith("core.")
            or module_name.startswith("tools.")
        ):
            imports_dict["local_imports"].append(module_name)
        else:
            # å‡è¨­æ˜¯ç¬¬ä¸‰æ–¹åº«
            imports_dict["third_party"].append(module_name)

    def _calculate_complexity_score(self, analysis: Dict) -> int:
        """è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸"""
        score = 0

        # åŸºæ–¼ä»£ç¢¼è¡Œæ•¸
        lines = analysis["lines_of_code"]
        if lines > 500:
            score += 50
        elif lines > 200:
            score += 20
        elif lines > 100:
            score += 10

        # åŸºæ–¼é¡åˆ¥æ•¸é‡
        score += len(analysis["classes"]) * 5

        # åŸºæ–¼å‡½æ•¸æ•¸é‡
        score += len(analysis["functions"]) * 2

        # TCKå„ªåŒ–ï¼šä½¿ç”¨sum()å…§å»ºå‡½æ•¸é€²è¡Œçµ±è¨ˆè¨ˆç®—
        total_imports = sum(
            len(analysis["imports"][category])
            for category in ["standard_library", "third_party", "local_imports"]
        )
        score += total_imports

        return score

    def check_dependency_issues(self) -> Dict[str, List]:
        """æª¢æŸ¥ä¾è³´å•é¡Œ"""
        print("\næª¢æŸ¥ä¾è³´å•é¡Œ...")

        issues = {"missing_imports": [], "circular_imports": [], "unused_imports": []}

        # æª¢æŸ¥æ¯å€‹æ–‡ä»¶çš„å°å…¥
        for file_path in self.python_files:
            try:
                with open(file_path, encoding="utf-8") as f:
                    content = f.read()

                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, (ast.Import, ast.ImportFrom)):
                        self._check_import_validity(node, file_path, issues)

            except Exception as e:
                issues["missing_imports"].append(
                    {
                        "file": str(file_path.relative_to(self.root_dir)),
                        "error": f"è§£æéŒ¯èª¤: {e}",
                    }
                )

        return issues

    def _check_import_validity(self, node, file_path, issues):
        """æª¢æŸ¥å°å…¥çš„æœ‰æ•ˆæ€§"""
        if isinstance(node, ast.Import):
            for alias in node.names:
                module_name = alias.name
                if not self._can_import_module(module_name):
                    issues["missing_imports"].append(
                        {
                            "file": str(file_path.relative_to(self.root_dir)),
                            "module": module_name,
                            "line": node.lineno,
                        }
                    )

        elif isinstance(node, ast.ImportFrom) and node.module:
            if node.level == 0:  # çµ•å°å°å…¥
                if not self._can_import_module(node.module):
                    issues["missing_imports"].append(
                        {
                            "file": str(file_path.relative_to(self.root_dir)),
                            "module": node.module,
                            "line": node.lineno,
                        }
                    )

    def _can_import_module(self, module_name: str) -> bool:
        """æª¢æŸ¥æ¨¡çµ„æ˜¯å¦å¯ä»¥å°å…¥"""
        try:
            importlib.import_module(module_name)
            return True
        except ImportError:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ¬åœ°æ¨¡çµ„
            if module_name in self.local_modules:
                return True
            # æª¢æŸ¥æ˜¯å¦ç‚ºcoreæ¨¡çµ„
            if module_name.startswith("core."):
                module_file = self.root_dir / f"{module_name.replace('.', '/')}.py"
                return module_file.exists()
            return False

    def find_duplicate_functionality(self) -> List[Dict]:
        """å°‹æ‰¾é‡è¤‡åŠŸèƒ½"""
        print("\nå°‹æ‰¾é‡è¤‡åŠŸèƒ½...")

        duplicates = []
        function_signatures = {}

        for analysis in self.analysis_results.values():
            if "error" in analysis:
                continue

            file_path = analysis["file_path"]

            # æª¢æŸ¥å‡½æ•¸é‡è¤‡
            for func in analysis["functions"]:
                signature = f"{func['name']}_{func['args_count']}"

                if signature in function_signatures:
                    duplicates.append(
                        {
                            "type": "function",
                            "name": func["name"],
                            "files": [function_signatures[signature], file_path],
                            "suggestion": f"è€ƒæ…®åˆä½µæˆ–é‡å‘½åå‡½æ•¸ '{func['name']}'",
                        }
                    )
                else:
                    function_signatures[signature] = file_path

            # TCKå„ªåŒ–ï¼šæª¢æŸ¥é¡åˆ¥é‡è¤‡ - ä½¿ç”¨é›†åˆæŸ¥æ‰¾é¿å…é‡è¤‡æ¯”è¼ƒ
            common_class_names = {"Config", "NovaSystem"}
            for cls in analysis["classes"]:
                if cls["name"] in common_class_names:
                    continue  # è·³éå¸¸è¦‹çš„é¡åˆ¥å

                # TCKå„ªåŒ–ï¼šæ”¶é›†æ‰€æœ‰å…¶ä»–æ–‡ä»¶çš„é¡åˆ¥åç¨±ä»¥é¿å…é‡è¤‡å·¢ç‹€æŸ¥æ‰¾
                other_class_names = set()
                for other_file, other_analysis in self.analysis_results.items():
                    if other_file != file_path and "classes" in other_analysis:
                        other_class_names.update(
                            other_cls["name"] for other_cls in other_analysis["classes"]
                        )

                if cls["name"] in other_class_names:
                    # æ‰¾åˆ°é‡è¤‡ï¼Œè¨˜éŒ„è©³ç´°ä¿¡æ¯
                    duplicate_files = [file_path]
                    for other_file, other_analysis in self.analysis_results.items():
                        if (
                            other_file != file_path
                            and "classes" in other_analysis
                            and any(
                                other_cls["name"] == cls["name"]
                                for other_cls in other_analysis["classes"]
                            )
                        ):
                            duplicate_files.append(other_analysis["file_path"])

                    duplicates.append(
                        {
                            "type": "class",
                            "name": cls["name"],
                            "files": duplicate_files,
                            "suggestion": f"è€ƒæ…®åˆä½µæˆ–é‡å‘½åé¡åˆ¥ '{cls['name']}'",
                        }
                    )

        return duplicates

    def analyze_all_files(self) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰æ–‡ä»¶"""
        print("Nova AI ä»£ç¢¼åˆ†æé–‹å§‹")
        print("=" * 60)

        # æƒææ–‡ä»¶
        self.scan_python_files()

        # åˆ†ææ¯å€‹æ–‡ä»¶
        print(f"\nåˆ†æ {len(self.python_files)} å€‹æ–‡ä»¶çš„è¤‡é›œåº¦...")

        for file_path in self.python_files:
            print(f"   åˆ†æ: {file_path.relative_to(self.root_dir)}")
            analysis = self.analyze_file_complexity(file_path)
            self.analysis_results[str(file_path)] = analysis

        # æª¢æŸ¥ä¾è³´å•é¡Œ
        dependency_issues = self.check_dependency_issues()

        # å°‹æ‰¾é‡è¤‡åŠŸèƒ½
        duplicates = self.find_duplicate_functionality()

        # ç”Ÿæˆç¸½çµ
        total_lines = sum(
            a.get("lines_of_code", 0) for a in self.analysis_results.values()
        )

        avg_complexity = (
            sum(a.get("complexity_score", 0) for a in self.analysis_results.values())
            / len(self.analysis_results)
            if self.analysis_results
            else 0
        )

        summary = {
            "total_files": len(self.python_files),
            "total_lines": total_lines,
            "average_complexity": round(avg_complexity, 2),
            "dependency_issues": len(dependency_issues["missing_imports"]),
            "duplicate_functions": len(
                [d for d in duplicates if d["type"] == "function"]
            ),
            "duplicate_classes": len([d for d in duplicates if d["type"] == "class"]),
        }

        results = {
            "summary": summary,
            "file_analyses": self.analysis_results,
            "dependency_issues": dependency_issues,
            "duplicates": duplicates,
            "timestamp": datetime.now().isoformat(),
        }

        # é¡¯ç¤ºç¸½çµ
        print("\n" + "=" * 60)
        print("ä»£ç¢¼åˆ†æç¸½çµ")
        print("=" * 60)
        print(f"ç¸½æ–‡ä»¶æ•¸: {summary['total_files']}")
        print(f"ç¸½ä»£ç¢¼è¡Œæ•¸: {summary['total_lines']}")
        print(f"å¹³å‡è¤‡é›œåº¦: {summary['average_complexity']}")
        print(f"ä¾è³´å•é¡Œ: {summary['dependency_issues']}")
        print(f"é‡è¤‡å‡½æ•¸: {summary['duplicate_functions']}")
        print(f"é‡è¤‡é¡åˆ¥: {summary['duplicate_classes']}")

        if duplicates:
            print(f"\nç™¼ç¾ {len(duplicates)} å€‹é‡è¤‡é …ç›®:")
            for dup in duplicates[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                print(f"   - {dup['type']}: {dup['name']} ({len(dup['files'])} å€‹æ–‡ä»¶)")

        return results
